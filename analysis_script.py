# -*- coding: utf-8 -*-
"""analysis_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J5IKEHhot_qd8BcCQqah1jF5C3686wYc
"""

import psycopg2
import pandas as pd
from datetime import datetime, timedelta
import numpy as np
import boto3
from decimal import Decimal
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import os
from io import StringIO

# Configuration

RDS_HOST = "YOUR_RDS_HOST_NAME"
RDS_DBNAME = "YOUR_RDS_DBNAME"
RDS_USER = "YOUR_RDS_USER"
RDS_PASSWORD = "YOUR_RDS_PASSWORD"

DYNAMODB_TABLE_NAME = "YOUR_DYNAMODB_TABLE_NAME"
AWS_REGION_NAME = "YOUR_REGION_NAME"

# Database Connection Functions

def connect_rds():
    """Establishes a connection to the RDS PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            host=RDS_HOST,
            database=RDS_DBNAME,
            user=RDS_USER,
            password=RDS_PASSWORD
        )
        print("Successfully connected to RDS.")
        return conn
    except Exception as e:
        print(f"Error connecting to RDS: {e}")
        return None

def run_sql_query(query, conn):
    """Executes an SQL query and returns results as a Pandas DataFrame."""
    try:
        with pd.option_context('mode.chained_assignment', None):
            df = pd.read_sql(query, conn)
        print("SQL Query executed successfully.")
        return df
    except Exception as e:
        print(f"Error executing SQL query: {e}")
        return None

def connect_dynamodb_resource():
    """Establishes a DynamoDB resource connection."""
    try:
        dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION_NAME)
        print("Successfully connected to DynamoDB resource.")
        return dynamodb
    except Exception as e:
        print(f"Error connecting to DynamoDB: {e}")
        return None

# DynamoDB Data Retrieval

def get_customer_behavior(dynamodb_resource, customer_id):
    """Retrieves behavioral data for a specific customer from DynamoDB."""
    table = dynamodb_resource.Table(DYNAMODB_TABLE_NAME)
    try:
        response = table.get_item(Key={'customer_id': Decimal(str(customer_id))})
        customer_behavior = response.get('Item')
        return customer_behavior
    except Exception as e:
        print(f"Error getting item for {customer_id} from DynamoDB: {e}")
        return None

def get_all_customer_behavior(dynamodb_resource):
    """Retrieves all customer behavioral data from DynamoDB (use with caution for large tables)."""
    table = dynamodb_resource.Table(DYNAMODB_TABLE_NAME)
    try:
        response = table.scan()
        items = response['Items']
        while 'LastEvaluatedKey' in response:
            response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])
            items.extend(response['Items'])
        print(f"Retrieved {len(items)} items from DynamoDB.")
        for item in items:
            for k, v in item.items():
                if isinstance(v, Decimal):
                    item[k] = float(v)
        return pd.DataFrame(items)
    except Exception as e:
        print(f"Error scanning DynamoDB: {e}")
        return pd.DataFrame()

# Feature Engineering

def engineer_features(transactions_df, customer_behavior_df):
    """
    Engineers new features for anomaly detection.
    Expects transactions_df from RDS and customer_behavior_df from DynamoDB.
    """
    if transactions_df.empty:
        print("No transaction data provided for feature engineering.")
        return pd.DataFrame()

    processed_df = transactions_df.copy()
    processed_df['trans_date_trans_time'] = pd.to_datetime(processed_df['trans_date_trans_time'])

    processed_df['transaction_hour'] = processed_df['trans_date_trans_time'].dt.hour

    processed_df['day_of_week'] = processed_df['trans_date_trans_time'].dt.dayofweek

    processed_df.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)

    customer_rolling_avg = processed_df.groupby('cc_num').apply(
        lambda x: x.set_index('trans_date_trans_time')['amt'].rolling('7D').mean(),
        include_groups=False
    ).reset_index()
    customer_rolling_avg.rename(columns={'amt': 'customer_avg_spending_last_7_days'}, inplace=True)
    processed_df = pd.merge(processed_df, customer_rolling_avg, on=['cc_num', 'trans_date_trans_time'], how='left')


    processed_df.sort_values(by=['merchant', 'trans_date_trans_time'], inplace=True)
    merchant_rolling_avg = processed_df.groupby('merchant').apply(
        lambda x: x.set_index('trans_date_trans_time')['amt'].rolling('30D').mean(),
        include_groups=False
    ).reset_index()
    merchant_rolling_avg.rename(columns={'amt': 'merchant_avg_spending_last_30_days'}, inplace=True)
    processed_df = pd.merge(processed_df, merchant_rolling_avg, on=['merchant', 'trans_date_trans_time'], how='left')


    processed_df['customer_avg_spending_last_7_days'] = processed_df['customer_avg_spending_last_7_days'].fillna(
        processed_df['amt'].mean()
    )
    processed_df['merchant_avg_spending_last_30_days'] = processed_df['merchant_avg_spending_last_30_days'].fillna(
        processed_df['amt'].mean()
    )

    if 'state' in processed_df.columns:
        processed_df['is_out_of_state'] = (processed_df['state'] != 'TX').astype(int)
    else:
        processed_df['is_out_of_state'] = 0

    if not customer_behavior_df.empty:
        customer_behavior_df['customer_id'] = pd.to_numeric(customer_behavior_df['customer_id'], errors='coerce')
        customer_behavior_df.rename(columns={'customer_id': 'cc_num'}, inplace=True)

        if 'latest_transaction_timestamp' in customer_behavior_df.columns:
            customer_behavior_df['latest_transaction_timestamp'] = pd.to_datetime(customer_behavior_df['latest_transaction_timestamp'], errors='coerce')

        processed_df['cc_num'] = processed_df['cc_num'].astype('int64')
        customer_behavior_df['cc_num'] = customer_behavior_df['cc_num'].astype('int64')

        processed_df = pd.merge(processed_df, customer_behavior_df[[
            'cc_num', 'total_spent_last_month', 'most_common_category', 'latest_transaction_timestamp'
        ]], on='cc_num', how='left')

        processed_df['category_deviation'] = (processed_df['category'] != processed_df['most_common_category']).astype(int)

        processed_df['total_spent_last_month'] = processed_df['total_spent_last_month'].fillna(0)
        processed_df['category_deviation'] = processed_df['category_deviation'].fillna(0)
        processed_df['time_since_last_dynamodb_update'] = (datetime.now() - processed_df['latest_transaction_timestamp']).dt.total_seconds() / 3600
        processed_df['time_since_last_dynamodb_update'] = processed_df['time_since_last_dynamodb_update'].fillna(processed_df['time_since_last_dynamodb_update'].max()) # Fill new customers with max value

    for col in processed_df.select_dtypes(include=np.number).columns:
        processed_df[col] = processed_df[col].fillna(processed_df[col].mean())

    print("\nFeatures Engineered:")
    print(processed_df[['cc_num', 'trans_date_trans_time', 'amt', 'transaction_hour', 'day_of_week',
                         'customer_avg_spending_last_7_days', 'merchant_avg_spending_last_30_days',
                         'is_out_of_state', 'total_spent_last_month', 'category_deviation']].head())
    return processed_df

# Anomaly Detection

def perform_anomaly_detection(processed_df):
    """
    Applies IsolationForest for unsupervised anomaly detection.
    Returns DataFrame with 'anomaly_score' and 'is_anomaly' columns.
    """
    if processed_df.empty:
        print("No processed data for anomaly detection.")
        return pd.DataFrame()

    features_for_anomaly = [
        'amt',
        'customer_avg_spending_last_7_days',
        'merchant_avg_spending_last_30_days',
        'transaction_hour',
        'day_of_week',
        'is_out_of_state',
        'total_spent_last_month',
        'category_deviation'
    ]

    features_for_anomaly_present = [col for col in features_for_anomaly if col in processed_df.columns]

    if not features_for_anomaly_present:
        print("No valid numerical features found for anomaly detection.")
        return processed_df

    X = processed_df[features_for_anomaly_present]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    model = IsolationForest(contamination=0.01, random_state=42, max_samples=0.8)
    model.fit(X_scaled)

    processed_df['anomaly_score'] = model.decision_function(X_scaled)
    processed_df['is_anomaly'] = model.predict(X_scaled)

    print("\nAnomaly Detection Results (Top Anomalies):")
    print(processed_df[['cc_num', 'trans_date_trans_time', 'amt', 'category', 'anomaly_score', 'is_anomaly']]
          .sort_values(by='anomaly_score').head(10))

    return processed_df

# Store Anomaly Results in RDS

def store_anomaly_results_in_rds(conn, processed_df):
    """
    Adds anomaly score and flag to the transactions table in RDS using a temporary table for efficiency.
    """
    if processed_df.empty:
        print("No processed data to store anomaly results.")
        return

    print("\nStoring Anomaly Detection Results in RDS...")
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                DO $$ BEGIN
                    IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='transactions' AND column_name='anomaly_score') THEN
                        ALTER TABLE transactions ADD COLUMN anomaly_score DECIMAL(10, 5);
                    END IF;
                    IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='transactions' AND column_name='is_anomaly') THEN
                        ALTER TABLE transactions ADD COLUMN is_anomaly INTEGER;
                    END IF;
                END $$;
            """)
            conn.commit()
            print("Anomaly columns checked/added in RDS.")

            cursor.execute("""
                CREATE TEMPORARY TABLE temp_anomaly_updates (
                    cc_num BIGINT,
                    trans_date_trans_time TIMESTAMP,
                    anomaly_score DECIMAL(10, 5),
                    is_anomaly INTEGER
                );
            """)
            conn.commit()
            print("Temporary table created.")

            data_to_copy = processed_df[[
                'cc_num', 'trans_date_trans_time', 'anomaly_score', 'is_anomaly'
            ]].copy()

            data_to_copy['cc_num'] = data_to_copy['cc_num'].astype('int64')
            data_to_copy['trans_date_trans_time'] = data_to_copy['trans_date_trans_time'].dt.strftime('%Y-%m-%d %H:%M:%S')
            data_to_copy['anomaly_score'] = data_to_copy['anomaly_score'].round(5)
            data_to_copy['is_anomaly'] = data_to_copy['is_anomaly'].astype(int)

            buffer = StringIO()
            data_to_copy.to_csv(buffer, sep='\t', header=False, index=False, na_rep='\\N')
            buffer.seek(0)

            cursor.copy_from(buffer, 'temp_anomaly_updates', columns=('cc_num', 'trans_date_trans_time', 'anomaly_score', 'is_anomaly'))
            conn.commit()
            print(f"Copied {len(data_to_copy)} rows to temporary table.")

            update_query = """
                UPDATE transactions AS t
                SET
                    anomaly_score = tau.anomaly_score,
                    is_anomaly = tau.is_anomaly
                FROM
                    temp_anomaly_updates AS tau
                WHERE
                    t.cc_num::BIGINT = tau.cc_num::BIGINT AND t.trans_date_trans_time = tau.trans_date_trans_time;
            """
            cursor.execute(update_query)
            conn.commit()
            print(f"Successfully updated anomaly flags for {cursor.rowcount} transactions in RDS.")


    except Exception as e:
        conn.rollback() # Rollback on error
        print(f"Error storing anomaly results in RDS: {e}")

# Export Data to CSV for Tableau Public
def export_rds_to_csv(conn, query, file_path):
    """
    Exports data from RDS to a CSV file.
    """
    try:
        with pd.option_context('mode.chained_assignment', None):
            df = pd.read_sql(query, conn)

        for col in df.select_dtypes(include=['datetime64[ns, UTC]', 'datetime64[ns]']).columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')

        df.to_csv(file_path, index=False)
        print(f"Data successfully exported to {file_path}")
        return True
    except Exception as e:
        print(f"Error exporting data to CSV: {e}")
        return False

# Main Execution Block

if __name__ == "__main__":
    print("--- Starting Analysis Script ---")

    rds_conn = connect_rds()
    dynamodb_resource = connect_dynamodb_resource()

    if rds_conn and dynamodb_resource:
        print("\n--- Running SQL Queries on RDS ---")
        query_all_transactions = "SELECT * FROM transactions ORDER BY trans_date_trans_time ASC LIMIT 50000;"
        df_all_transactions = run_sql_query(query_all_transactions, rds_conn)

        if not df_all_transactions.empty:
            print("\nFirst 5 rows of all transactions from RDS:")
            print(df_all_transactions.head())

            query_spending_by_category = """
            SELECT
                category,
                SUM(amt) AS total_amount,
                COUNT(cc_num) AS num_transactions
            FROM
                transactions
            WHERE
                trans_date_trans_time >= NOW() - INTERVAL '1 month'
            GROUP BY
                category
            ORDER BY
                total_amount DESC;
            """
            df_spending_by_category = run_sql_query(query_spending_by_category, rds_conn)
            if df_spending_by_category is not None:
                print("\nAggregated Spending by Category (Last Month):")
                print(df_spending_by_category.head())

            query_avg_trans_per_customer = """
            SELECT
                cc_num,
                AVG(amt) AS avg_trans_amt,
                COUNT(cc_num) AS total_trans
            FROM
                transactions
            GROUP BY
                cc_num
            ORDER BY
                avg_trans_amt DESC;
            """
            df_avg_trans_per_customer = run_sql_query(query_avg_trans_per_customer, rds_conn)
            if df_avg_trans_per_customer is not None:
                print("\nAverage Transaction Amount Per Customer:")
                print(df_avg_trans_per_customer.head())
        else:
            print("No transactions loaded from RDS. Skipping further analysis steps.")


        print("\n--- Running DynamoDB Queries ---")
        sample_customer_id = 2703186189652095

        behavior_data_single = get_customer_behavior(dynamodb_resource, sample_customer_id)
        if behavior_data_single:
            print(f"\nBehavioral data for {sample_customer_id}:")
            print(behavior_data_single)
        else:
            print(f"No behavioral data found for {sample_customer_id}. Please check the sample_customer_id.")

        df_customer_behavior = get_all_customer_behavior(dynamodb_resource)
        if not df_customer_behavior.empty:
            print("\nFirst 5 rows of customer behavior from DynamoDB:")
            print(df_customer_behavior.head())


        if not df_all_transactions.empty:
            print("\n--- Starting Feature Engineering ---")
            df_processed = engineer_features(df_all_transactions, df_customer_behavior)

            if not df_processed.empty:
                print("\n--- Starting Anomaly Detection Model ---")
                df_results = perform_anomaly_detection(df_processed)

                if rds_conn:
                    store_anomaly_results_in_rds(rds_conn, df_results)

                    # Export the full transactions table with anomaly flags to CSV
                    print("\n--- Exporting RDS data to CSV for Tableau Public ---")
                    export_query = """
                    SELECT
                        trans_date_trans_time, cc_num, merchant, category, amt, city, state,
                        anomaly_score, is_anomaly
                    FROM transactions
                    ORDER BY trans_date_trans_time ASC
                    LIMIT 50000;
                    """
                    csv_file_path = "/home/ec2-user/transactions_with_anomalies.csv"
                    export_rds_to_csv(rds_conn, export_query, csv_file_path)

            else:
                print("Skipping anomaly detection and storage due to empty processed DataFrame.")
        else:
            print("Skipping feature engineering, anomaly detection, and storage as no transactions were loaded.")


    if rds_conn:
        rds_conn.close()
    print("RDS connection closed.")

    print("\n--- Analysis Script Finished ---")